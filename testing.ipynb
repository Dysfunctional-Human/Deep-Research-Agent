{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "426fc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SubGraphs.AnalystsGraph import AnalystGraph\n",
    "\n",
    "Team = AnalystGraph()\n",
    "graph = Team.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc26091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfc8f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysts = Team.create_analysts(state={\"topic\": \"Large Language Models\", \"max_analysts\": 3, \"human_analyst_feedback\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48012073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analystTeam': [Analyst(affiliation='AI Research Institute', name='Dr. Anya Sharma', role='Chief AI Architect', description='Focuses on the cutting-edge technical innovations in LLM architectures, training methodologies, and performance optimization. Concerned with scalability, efficiency, and pushing the boundaries of what LLMs can achieve.'),\n",
       "  Analyst(affiliation='Center for Digital Ethics', name='Professor Ben Carter', role='Lead Ethicist & Sociologist', description='Investigates the ethical dilemmas, societal implications, and potential biases embedded within Large Language Models. Concerned with fairness, transparency, accountability, and ensuring responsible development and deployment of AI.'),\n",
       "  Analyst(affiliation='Tech Market Insights Group', name='Ms. Chloe Davis', role='Senior Market Strategist', description='Analyzes the commercial applications, market trends, and economic impact of Large Language Models across various industries. Concerned with competitive landscapes, investment opportunities, and the strategic adoption of LLM technologies by enterprises.')],\n",
       " 'messages': ['Large Language Models']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6a4c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SubGraphs.InterviewGraph import InterviewAgent\n",
    "\n",
    "interview_graph = InterviewAgent()\n",
    "graph = interview_graph.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb0b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aarav\\OneDrive\\Desktop\\Deep Research Agent\\SubGraphs\\InterviewGraph.py:104: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  tavilly_search = TavilySearchResults(max_results=3)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "topic = \"Large Language Models\"\n",
    "input = {\n",
    "    'max_num_turns': 2,\n",
    "    'analyst': analysts['analystTeam'][0],\n",
    "    \"messages\": [HumanMessage(f\"So you said you were writing an article on {topic}?\")]\n",
    "}\n",
    "\n",
    "section = graph.invoke(input, config={\"thread_id\": \"1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1c98558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='So you said you were writing an article on Large Language Models?', additional_kwargs={}, response_metadata={}, id='f6298d16-9ae3-441e-a1f0-e7c297eedcda'),\n",
       "  AIMessage(content=\"Hello Dr. Sharma, it's a pleasure to meet you. I'm Alex Chen, an analyst, and yes, I am indeed working on an article about Large Language Models. My focus is really on understanding the bleeding edge of LLM development – not just what they can do today, but where the technology is headed and what technical hurdles are being overcome.\\n\\nGiven your role as Chief AI Architect at the AI Research Institute, focusing on architecture, training, and optimization, I'm particularly keen to hear your perspective. Could you perhaps start by telling me about one or two of the most significant, perhaps non-obvious, technical innovations in LLM architectures or training methodologies that you've seen emerge in the last year or so? What's truly pushing the boundaries beyond just scaling up existing models?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--8309e7e0-a99e-4257-a35d-16b324f3dcbc-0', usage_metadata={'input_tokens': 237, 'output_tokens': 298, 'total_tokens': 535, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 133}}),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, name='expert', id='lc_run--f94ea5d6-e627-446f-a607-cf76f8f6709b-0', usage_metadata={'input_tokens': 1104, 'output_tokens': 0, 'total_tokens': 1104, 'input_token_details': {'cache_read': 0}}),\n",
       "  AIMessage(content='Dr. Sharma smiles, leaning forward slightly. \"It\\'s a pleasure to meet you too, Alex. Your focus is precisely where much of our most exciting work lies. It\\'s easy to get caught up in the sheer scale of these models, but the real breakthroughs often come from rethinking fundamental assumptions.\"\\n\\n\"If I had to pick one innovation that\\'s truly transformative and perhaps not immediately obvious to everyone, it would be the widespread adoption and refinement of **Mixture-of-Experts (MoE) architectures**. For a long time, the prevailing wisdom was that to get better performance, you needed denser, larger models where every parameter was active for every token. MoE flips that on its head.\"\\n\\n\"What\\'s non-obvious about it is that you can build models with *trillions* of parameters, yet during inference, only a small fraction of those parameters are actually activated for any given input. Imagine a model with 100 different \\'expert\\' subnetworks. When you feed it a prompt, a \\'router\\' network decides which 2 or 3 experts are most relevant to process that specific piece of information. This means you get the representational power of a massive model, but the computational cost of a much smaller one.\"\\n\\n\"For instance, models like Google\\'s Gemini 1.5 or Mistral\\'s Mixtral 8x7B leverage this. Mixtral, despite having 45 billion total parameters, only activates about 13 billion during inference. This translates directly into significantly faster inference speeds and lower operational costs compared to a dense model of equivalent performance, or allows us to train models that would otherwise be computationally intractable. The challenge, of course, lies in effective routing and ensuring load balancing across these experts during training and inference, which is where a lot of the cutting-edge research is focused right now.\"\\n\\n\"Another area, perhaps less about architecture and more about training methodology, is the sophisticated use of **synthetic data generation and curriculum learning for alignment**. Beyond just RLHF, we\\'re seeing techniques where models generate their own training data, or where the training process is structured in stages, gradually introducing more complex tasks or nuanced ethical considerations. This allows for much more targeted and efficient alignment, especially for specialized domains, without relying solely on vast amounts of human-annotated data, which can be slow and expensive.\"', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--9b66f558-3b0d-4f37-8af0-133c3a34984e-0', usage_metadata={'input_tokens': 404, 'output_tokens': 1247, 'total_tokens': 1651, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 761}}),\n",
       "  AIMessage(content=['\\n\\n\"But let\\'s start with MoE. What are your thoughts on its implications for the future of LLM scalability and efficiency, particularly from a technical architecture standpoint?\"', 'That\\'s an excellent point to dive deeper into, Dr. Sharma. From a technical architecture standpoint, the implications of Mixture-of-Experts (MoE) for the future of LLM scalability and efficiency are profound.\\n\\nFirstly, MoE represents a \"key architectural innovation\" [5] and an \"essential technique\" [3] that fundamentally redefines how we approach model capacity. By splitting computation into multiple \"expert subnetworks\" [4], MoE models can significantly \"increase model capacity by replacing layers with MoE layers\" [4]. This allows for the development of \"hyper-scaled models that leverage innovative techniques to push the boundaries of what is computationally feasible and functionally possible\" [3]. Essentially, it enables us to build models with a much larger total number of parameters than would be practical with dense architectures, thereby enhancing their overall capabilities and range of applications [3].\\n\\nSecondly, and critically for efficiency, sparse MoE models are \"more flop-efficient per parameter used, reducing compute costs compared to dense models of similar size\" [4]. This efficiency stems from the fact that only a subset of these expert subnetworks is activated for any given input [4]. This architectural choice allows for the creation of \"incredibly powerful yet surprisingly efficient language models\" [5]. The ability to achieve high performance with lower computational overhead per inference is vital for deploying and scaling LLMs, especially as models continue to grow in complexity and size [3, 4].\\n\\nIn essence, MoE architectures provide a pathway to simultaneously achieve greater scale and improved efficiency, which is a critical balance for pushing the boundaries of what LLMs can achieve in real-world applications.\\n\\n**Sources:**\\n[1] https://arxiv.org/pdf/2503.22732\\n[2] https://www.sciencedirect.com/science/article/pii/S2666764925000323\\n[3] https://www.mdpi.com/2673-4591/97/1/8\\n[4] https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/\\n[5] https://medium.com/google-cloud/how-mixture-of-experts-llms-work-58b3ba8e0349'], additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, name='expert', id='lc_run--15d14356-11d0-4d0a-8df5-99dddd7e5ee1-0', usage_metadata={'input_tokens': 2457, 'output_tokens': 1941, 'total_tokens': 4398, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1408}})],\n",
       " 'max_num_turns': 2,\n",
       " 'context': ['<Document href=\"https://arxiv.org/pdf/2503.22732\"/>\\nIII. TOP LLM MODELS Recent advances in LLMs during 2023–2025 have demon-strated remarkable progress in architecture and training methodologies. For example, UI-TARS  and T¨ ulu 3  exemplify novel approaches in multimodal processing, and transparent post-training pipelines, respectively, that have en-abled significant improvements in reasoning, code genera-tion, and domain-specific tasks. These developments leverage innovations such as reinforcement learning with verifiable rewards, [...] various aspects, including architectural innovations, training strategies (e.g., fine-tuning, reinforcement learning, and in-context learning), and evaluation benchmarks. [...] By systematically examining these state-of-the-art models, we aim to elucidate the underlying innovations in architecture, training methodologies, and application domains that collec-tively contribute to their enhanced performance. The mod-els discussed herein represent breakthroughs in multimodal integration, reasoning, and code generation and underscore the trend toward increased transparency and reproducibility in AI research. This detailed survey serves as a foundation for understanding the\\n</Document>\\n\\n---\\n\\n<Document href=\"https://www.sciencedirect.com/science/article/pii/S2666764925000323\"/>\\n1. Over time, LLMs have evolved from earlier iterations of the GPT series to GPT-4/5 and beyond, growing in scale and introducing innovations in architecture and training techniques to tackle increasingly complex language understanding and generation tasks.\\n</Document>\\n\\n---\\n\\n<Document href=\"https://www.mdpi.com/2673-4591/97/1/8\"/>\\nThe evolution of large language models (LLMs) has been marked by significant architectural and methodological breakthroughs that have redefined the landscape of natural language processing. This review examines the key techniques driving modern LLMs, including foundational architectures, novel training methodologies, and cutting-edge performance benchmarks. In addition to offering a performance overview, this work presents a focused and up-to-date architectural benchmark that highlights key [...] The architecture of LLMs serves as the foundational framework that dictates their capabilities, efficiency, and range of applications. Over the years, a multitude of architectures and techniques have been proposed, each with unique strengths and limitations. Recent advancements have ushered in an era of hyper-scaled models that leverage innovative techniques to push the boundaries of what is computationally feasible and functionally possible. [...] ## 4. Conclusions\\n\\nIn the rapidly evolving landscape of large language models (LLMs), understanding their underlying architectures, training strategies, and key technical advances is crucial. This paper has provided a concise yet comprehensive overview of recent progress in open-source LLMs, highlighting essential techniques such as mixture-of-experts, advanced attention mechanisms, normalization layers, and scalable training methodologies.\\n</Document>',\n",
       "  '',\n",
       "  '<Document href=\"https://www.computer.org/publications/tech-news/trends/training-techniques-large-language-models\"/>\\nTo overcome these issues, future research will focus on developing more efficient and environmentally friendly training methods, creating more diverse and representative datasets, and improving our ability to understand and explain what LLMs are doing. Collaboration between researchers, practitioners, and policymakers will be essential to unlock the full potential of LLMs.\\n\\n## Conclusion [...] The ongoing development of more effective training methods for LLMs is a key area of research with important consequences for the future of natural language processing. As models get larger and more complex, creating new and better training methods is increasingly essential.\\n\\n## Challenges and Future Directions [...] # Innovations in Training Techniques for Large Language Models\\n\\nBy Lalit Choureyon\\n\\nLarge language models (LLMs) have significantly advanced natural language processing (NLP), excelling in tasks such as text generation, translation, and summarization. These models are trained on vast datasets, enabling them to recognize complex language patterns. However, their large size presents challenges, requiring significant computational power, lengthy training periods, and extensive data.\\n</Document>\\n\\n---\\n\\n<Document href=\"https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/\"/>\\nMixture of Experts (MoE) is an architectural pattern for neural networks that splits computation into multiple expert subnetworks, which are combined to create the final output.\\n MoE models can increase model capacity by replacing layers with MoE layers, and sparse MoE models are more flop-efficient per parameter used, reducing compute costs compared to dense models of similar size. [...] A mixture of experts is an architectural pattern for neural networks that splits the computation of a layer or operation (such as linear layers, MLPs, or attention projection) into multiple “expert” subnetworks. These subnetworks each independently perform their own computation, the results of which are combined to create the final output of the MoE layer. MoE architectures can be either dense, meaning that every expert is used in the case of every input, or sparse, meaning that a subset of [...] ### Breaking Through Reinforcement Learning Training Limits with Scaling Rollouts in BroRL\\n\\n Breaking Through Reinforcement Learning Training Limits with Scaling Rollouts in BroRL\\n\\n### Pioneering AI Co-Scientists for Fusion Research and Cancer Treatment\\n\\n Pioneering AI Co-Scientists for Fusion Research and Cancer Treatment\\n\\n### Achieve CUTLASS C++ Performance with Python APIs Using CuTe DSL\\n\\n Achieve CUTLASS C++ Performance with Python APIs Using CuTe DSL\\n</Document>\\n\\n---\\n\\n<Document href=\"https://medium.com/google-cloud/how-mixture-of-experts-llms-work-58b3ba8e0349\"/>\\nWith an LLM, the training method is completely different. Training an LLM uses text pulled from whatever sources were used, and the process is as simple as removing the last word of the text. Once that is done, the initial part of the text is used as input, and the output is then compared to the chopped-off last word. [...] Although many companies that provide commercial models (like Google or OpenAI) typically do not reveal details of their model’s architecture or training methods, there are a number of well-known models that do use the MoE approach, including Grok-1, DeepSeek, Mixtral, and Qwen1.5-MoE. Additionally, it is believed that models like GPT-4, PaLM2 and Claude use an approach similar to MoE.\\n\\n## Conclusion and Additional Resources [...] This is where Mixture of Expert (MoE) LLM models provide a key architectural innovation. This article will demystify MoE architectures, explaining in plain English how they allow for the creation of incredibly powerful yet surprisingly efficient language models that are helping to shape the future of AI.\\n\\n## The Evolution of Large Language Models\\n</Document>',\n",
       "  ''],\n",
       " 'analyst': Analyst(affiliation='AI Research Institute', name='Dr. Anya Sharma', role='Chief AI Architect', description='Focuses on the cutting-edge technical innovations in LLM architectures, training methodologies, and performance optimization. Concerned with scalability, efficiency, and pushing the boundaries of what LLMs can achieve.'),\n",
       " 'interview': 'Human: So you said you were writing an article on Large Language Models?\\nAI: Hello Dr. Sharma, it\\'s a pleasure to meet you. I\\'m Alex Chen, an analyst, and yes, I am indeed working on an article about Large Language Models. My focus is really on understanding the bleeding edge of LLM development – not just what they can do today, but where the technology is headed and what technical hurdles are being overcome.\\n\\nGiven your role as Chief AI Architect at the AI Research Institute, focusing on architecture, training, and optimization, I\\'m particularly keen to hear your perspective. Could you perhaps start by telling me about one or two of the most significant, perhaps non-obvious, technical innovations in LLM architectures or training methodologies that you\\'ve seen emerge in the last year or so? What\\'s truly pushing the boundaries beyond just scaling up existing models?\\nAI: \\nAI: Dr. Sharma smiles, leaning forward slightly. \"It\\'s a pleasure to meet you too, Alex. Your focus is precisely where much of our most exciting work lies. It\\'s easy to get caught up in the sheer scale of these models, but the real breakthroughs often come from rethinking fundamental assumptions.\"\\n\\n\"If I had to pick one innovation that\\'s truly transformative and perhaps not immediately obvious to everyone, it would be the widespread adoption and refinement of **Mixture-of-Experts (MoE) architectures**. For a long time, the prevailing wisdom was that to get better performance, you needed denser, larger models where every parameter was active for every token. MoE flips that on its head.\"\\n\\n\"What\\'s non-obvious about it is that you can build models with *trillions* of parameters, yet during inference, only a small fraction of those parameters are actually activated for any given input. Imagine a model with 100 different \\'expert\\' subnetworks. When you feed it a prompt, a \\'router\\' network decides which 2 or 3 experts are most relevant to process that specific piece of information. This means you get the representational power of a massive model, but the computational cost of a much smaller one.\"\\n\\n\"For instance, models like Google\\'s Gemini 1.5 or Mistral\\'s Mixtral 8x7B leverage this. Mixtral, despite having 45 billion total parameters, only activates about 13 billion during inference. This translates directly into significantly faster inference speeds and lower operational costs compared to a dense model of equivalent performance, or allows us to train models that would otherwise be computationally intractable. The challenge, of course, lies in effective routing and ensuring load balancing across these experts during training and inference, which is where a lot of the cutting-edge research is focused right now.\"\\n\\n\"Another area, perhaps less about architecture and more about training methodology, is the sophisticated use of **synthetic data generation and curriculum learning for alignment**. Beyond just RLHF, we\\'re seeing techniques where models generate their own training data, or where the training process is structured in stages, gradually introducing more complex tasks or nuanced ethical considerations. This allows for much more targeted and efficient alignment, especially for specialized domains, without relying solely on vast amounts of human-annotated data, which can be slow and expensive.\"\\nAI: \\n\\n\"But let\\'s start with MoE. What are your thoughts on its implications for the future of LLM scalability and efficiency, particularly from a technical architecture standpoint?\"That\\'s an excellent point to dive deeper into, Dr. Sharma. From a technical architecture standpoint, the implications of Mixture-of-Experts (MoE) for the future of LLM scalability and efficiency are profound.\\n\\nFirstly, MoE represents a \"key architectural innovation\" [5] and an \"essential technique\" [3] that fundamentally redefines how we approach model capacity. By splitting computation into multiple \"expert subnetworks\" [4], MoE models can significantly \"increase model capacity by replacing layers with MoE layers\" [4]. This allows for the development of \"hyper-scaled models that leverage innovative techniques to push the boundaries of what is computationally feasible and functionally possible\" [3]. Essentially, it enables us to build models with a much larger total number of parameters than would be practical with dense architectures, thereby enhancing their overall capabilities and range of applications [3].\\n\\nSecondly, and critically for efficiency, sparse MoE models are \"more flop-efficient per parameter used, reducing compute costs compared to dense models of similar size\" [4]. This efficiency stems from the fact that only a subset of these expert subnetworks is activated for any given input [4]. This architectural choice allows for the creation of \"incredibly powerful yet surprisingly efficient language models\" [5]. The ability to achieve high performance with lower computational overhead per inference is vital for deploying and scaling LLMs, especially as models continue to grow in complexity and size [3, 4].\\n\\nIn essence, MoE architectures provide a pathway to simultaneously achieve greater scale and improved efficiency, which is a critical balance for pushing the boundaries of what LLMs can achieve in real-world applications.\\n\\n**Sources:**\\n[1] https://arxiv.org/pdf/2503.22732\\n[2] https://www.sciencedirect.com/science/article/pii/S2666764925000323\\n[3] https://www.mdpi.com/2673-4591/97/1/8\\n[4] https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/\\n[5] https://medium.com/google-cloud/how-mixture-of-experts-llms-work-58b3ba8e0349',\n",
       " 'sections': ['## Architectural Frontiers and Training Paradigms: Pushing LLM Boundaries for Hyper-Scale Efficiency\\n\\n### Summary\\nThe rapid evolution of Large Language Models (LLMs) continues to redefine the landscape of natural language processing, driven by a relentless pursuit of architectural innovation and advanced training methodologies. From early GPT iterations to the current state-of-the-art, LLMs have grown in scale and complexity, demonstrating remarkable progress in tackling increasingly intricate language understanding and generation tasks [2, 3]. This continuous advancement is fueled by a demand for greater efficiency, scalability, and capability, necessitating novel approaches to overcome the significant computational power and extensive data requirements inherent in these models [3, 4].\\n\\nA particularly impactful and novel architectural breakthrough is the Mixture of Experts (MoE) pattern [3, 5, 6]. MoE models significantly enhance LLM capacity by distributing computation across multiple \"expert\" subnetworks, whose outputs are then combined to produce the final result [5]. This design enables the creation of incredibly powerful yet surprisingly efficient models, especially in sparse MoE configurations, where only a subset of experts is activated for a given input. This approach substantially reduces compute costs per parameter compared to dense models of similar size [5]. Prominent models such as Grok-1, DeepSeek, Mixtral, and Qwen1.5-MoE openly leverage MoE, and it is widely believed that models like GPT-4, PaLM2, and Claude also employ similar architectural strategies [6].\\n\\nBeyond architectural shifts, training methodologies are also undergoing significant innovation. While foundational techniques like fine-tuning and in-context learning remain crucial, new strategies such as reinforcement learning with verifiable rewards are emerging. These advanced training methods are enabling significant improvements in reasoning, code generation, and domain-specific tasks [1]. Future research is keenly focused on developing more efficient and environmentally friendly training methods, alongside the creation of diverse and representative datasets to further enhance model capabilities and reduce resource consumption [4]. Recent models like UI-TARS and Tülu 3 exemplify these advancements, showcasing breakthroughs in multimodal integration and transparent post-training pipelines [1]. These developments collectively underscore a trend towards hyper-scaled models that push the boundaries of what is computationally feasible and functionally possible, while also emphasizing increased transparency and reproducibility in AI research [1, 3].\\n\\n### Sources\\n[1] https://arxiv.org/pdf/2503.22732  \\n[2] https://www.sciencedirect.com/science/article/pii/S2666764925000323  \\n[3] https://www.mdpi.com/2673-4591/97/1/8  \\n[4] https://www.computer.org/publications/tech-news/trends/training-techniques-large-language-models  \\n[5] https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/  \\n[6] https://medium.com/google-cloud/how-mixture-of-experts-llms-work-58b3ba8e0349']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259e660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Research Agent (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
